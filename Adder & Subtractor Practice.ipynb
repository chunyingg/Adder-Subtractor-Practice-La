{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSAI HW2 : Addar & Subtractor Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suchunying/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/Users/suchunying/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suchunying/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/suchunying/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定參數\n",
    "### 定義訓練集大小以及加入數字，分別設立正號跟負號個別的chars\n",
    "### 給定RNN所需要的參數size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 80000\n",
    "DIGITS = 3\n",
    "REVERSE = False\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "chars = '0123456789+ '\n",
    "chars2 = '0123456789- '\n",
    "chars3 = '0123456789+- '\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    def __init__(self, chars):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "    \n",
    "    def encode(self, C, num_rows):\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[i] for i in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctable = CharacterTable(chars)\n",
    "ctable2 = CharacterTable(chars2)\n",
    "ctable3 = CharacterTable(chars3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: '+',\n",
       " 2: '-',\n",
       " 3: '0',\n",
       " 4: '1',\n",
       " 5: '2',\n",
       " 6: '3',\n",
       " 7: '4',\n",
       " 8: '5',\n",
       " 9: '6',\n",
       " 10: '7',\n",
       " 11: '8',\n",
       " 12: '9'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctable3.indices_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------加法器---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation (adder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生訓練集以及測試集，question設定為方程式A+B，expected則為相對應的答案\n",
    "### 並將資料轉化為one-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 80000\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['647+3  ', '36+9   ', '634+1  ', '26+35  ', '61+96  '] ['650 ', '45  ', '635 ', '61  ', '157 ']\n"
     ]
    }
   ],
   "source": [
    "print(questions[:5], expected[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training size 18000 ， validation size 2000，test size 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(expected), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "(18000, 7, 12)\n",
      "(18000, 4, 12)\n",
      "Validation Data:\n",
      "(2000, 7, 12)\n",
      "(2000, 4, 12)\n",
      "Testing Data:\n",
      "(60000, 7, 12)\n",
      "(60000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# train_test_split\n",
    "train_x = x[:20000]\n",
    "train_y = y[:20000]\n",
    "test_x = x[20000:]\n",
    "test_y = y[20000:]\n",
    "\n",
    "split_at = len(train_x) - len(train_x) // 10\n",
    "(x_train, x_val) = train_x[:split_at], train_x[split_at:]\n",
    "(y_train, y_val) = train_y[:split_at], train_y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "print('Testing Data:')\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [[[False False False False False False False False False  True False\n",
      "   False]\n",
      "  [False False False False False False False False False False  True\n",
      "   False]\n",
      "  [False False False False False False False False False False False\n",
      "    True]\n",
      "  [False  True False False False False False False False False False\n",
      "   False]\n",
      "  [False False False False False False False False False False  True\n",
      "   False]\n",
      "  [False False False False False False False False  True False False\n",
      "   False]\n",
      "  [ True False False False False False False False False False False\n",
      "   False]]\n",
      "\n",
      " [[False False False  True False False False False False False False\n",
      "   False]\n",
      "  [False False False False False False False False False False False\n",
      "    True]\n",
      "  [False False False False  True False False False False False False\n",
      "   False]\n",
      "  [False  True False False False False False False False False False\n",
      "   False]\n",
      "  [False False False  True False False False False False False False\n",
      "   False]\n",
      "  [False False False False False False False False False False False\n",
      "    True]\n",
      "  [ True False False False False False False False False False False\n",
      "   False]]\n",
      "\n",
      " [[False False False False False False  True False False False False\n",
      "   False]\n",
      "  [False False False  True False False False False False False False\n",
      "   False]\n",
      "  [False False False False False False False False False False  True\n",
      "   False]\n",
      "  [False  True False False False False False False False False False\n",
      "   False]\n",
      "  [False False False False  True False False False False False False\n",
      "   False]\n",
      "  [False False False False False False False False False False False\n",
      "    True]\n",
      "  [ True False False False False False False False False False False\n",
      "   False]]] \n",
      "\n",
      " label:  [[[False False False False False False False False False False  True\n",
      "   False]\n",
      "  [False False False False False False False False False  True False\n",
      "   False]\n",
      "  [False False False False False False False  True False False False\n",
      "   False]\n",
      "  [ True False False False False False False False False False False\n",
      "   False]]\n",
      "\n",
      " [[False False False False  True False False False False False False\n",
      "   False]\n",
      "  [False False False  True False False False False False False False\n",
      "   False]\n",
      "  [False False False  True False False False False False False False\n",
      "   False]\n",
      "  [ True False False False False False False False False False False\n",
      "   False]]\n",
      "\n",
      " [[False False False False False False  True False False False False\n",
      "   False]\n",
      "  [False False False False False False  True False False False False\n",
      "   False]\n",
      "  [False False False False False False False False False  True False\n",
      "   False]\n",
      "  [ True False False False False False False False False False False\n",
      "   False]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"input: \", x_train[:3], '\\n\\n', \"label: \", y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定RNN model\n",
    "* activation為softmax\n",
    "* loss function為categorical_crossentropy\n",
    "* optimizer為adam\n",
    "* metrics為accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4, 12)             1548      \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "for _ in range(LAYERS):\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 135us/step - loss: 0.8190 - acc: 0.6962 - val_loss: 0.8286 - val_acc: 0.6847\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 130us/step - loss: 0.7457 - acc: 0.7268 - val_loss: 0.7502 - val_acc: 0.7101\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 135us/step - loss: 0.6642 - acc: 0.7615 - val_loss: 0.6798 - val_acc: 0.7305\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 132us/step - loss: 0.5826 - acc: 0.8018 - val_loss: 0.5789 - val_acc: 0.7985\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 130us/step - loss: 0.5145 - acc: 0.8344 - val_loss: 0.5170 - val_acc: 0.8224\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 139us/step - loss: 0.4650 - acc: 0.8562 - val_loss: 0.4805 - val_acc: 0.8388\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 139us/step - loss: 0.4189 - acc: 0.8765 - val_loss: 0.4410 - val_acc: 0.8579\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 136us/step - loss: 0.3683 - acc: 0.9041 - val_loss: 0.3898 - val_acc: 0.8838\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 146us/step - loss: 0.3399 - acc: 0.9124 - val_loss: 0.3991 - val_acc: 0.8774\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 140us/step - loss: 0.3075 - acc: 0.9253 - val_loss: 0.3378 - val_acc: 0.8980\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 157us/step - loss: 0.2721 - acc: 0.9399 - val_loss: 0.3203 - val_acc: 0.8987\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 176us/step - loss: 0.2520 - acc: 0.9431 - val_loss: 0.2666 - val_acc: 0.9316\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 168us/step - loss: 0.2278 - acc: 0.9521 - val_loss: 0.2657 - val_acc: 0.9308\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 160us/step - loss: 0.1992 - acc: 0.9621 - val_loss: 0.2281 - val_acc: 0.9455\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 152us/step - loss: 0.1820 - acc: 0.9663 - val_loss: 0.2151 - val_acc: 0.9421\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 151us/step - loss: 0.1718 - acc: 0.9665 - val_loss: 0.1977 - val_acc: 0.9507\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 179us/step - loss: 0.1474 - acc: 0.9748 - val_loss: 0.1769 - val_acc: 0.9559\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 174us/step - loss: 0.1535 - acc: 0.9674 - val_loss: 0.1872 - val_acc: 0.9473\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 145us/step - loss: 0.1286 - acc: 0.9780 - val_loss: 0.1491 - val_acc: 0.9636\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 174us/step - loss: 0.1164 - acc: 0.9807 - val_loss: 0.1515 - val_acc: 0.9589\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 180us/step - loss: 0.1085 - acc: 0.9827 - val_loss: 0.1539 - val_acc: 0.9597\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 158us/step - loss: 0.1214 - acc: 0.9726 - val_loss: 0.1475 - val_acc: 0.9570\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 162us/step - loss: 0.0950 - acc: 0.9847 - val_loss: 0.1314 - val_acc: 0.9654\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 147us/step - loss: 0.0857 - acc: 0.9869 - val_loss: 0.1148 - val_acc: 0.9723\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 145us/step - loss: 0.0776 - acc: 0.9878 - val_loss: 0.1075 - val_acc: 0.9730\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 148us/step - loss: 0.0676 - acc: 0.9913 - val_loss: 0.0980 - val_acc: 0.9775\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 149us/step - loss: 0.0647 - acc: 0.9912 - val_loss: 0.0922 - val_acc: 0.9752\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 154us/step - loss: 0.0760 - acc: 0.9843 - val_loss: 0.1359 - val_acc: 0.9515\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 170us/step - loss: 0.1158 - acc: 0.9672 - val_loss: 0.0981 - val_acc: 0.9726\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 157us/step - loss: 0.0553 - acc: 0.9923 - val_loss: 0.0775 - val_acc: 0.9805\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 154us/step - loss: 0.0442 - acc: 0.9958 - val_loss: 0.0756 - val_acc: 0.9808\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 157us/step - loss: 0.0421 - acc: 0.9958 - val_loss: 0.0797 - val_acc: 0.9775\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 173us/step - loss: 0.0418 - acc: 0.9948 - val_loss: 0.0697 - val_acc: 0.9822\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 141us/step - loss: 0.0378 - acc: 0.9960 - val_loss: 0.0847 - val_acc: 0.9738\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 156us/step - loss: 0.0505 - acc: 0.9901 - val_loss: 0.0769 - val_acc: 0.9764\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 131us/step - loss: 0.0334 - acc: 0.9967 - val_loss: 0.0611 - val_acc: 0.9835\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 133us/step - loss: 0.0345 - acc: 0.9953 - val_loss: 0.0847 - val_acc: 0.9720\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 137us/step - loss: 0.1118 - acc: 0.9654 - val_loss: 0.2191 - val_acc: 0.9184\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 140us/step - loss: 0.0604 - acc: 0.9848 - val_loss: 0.0588 - val_acc: 0.9867\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 176us/step - loss: 0.0262 - acc: 0.9979 - val_loss: 0.0543 - val_acc: 0.9857\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 174us/step - loss: 0.0228 - acc: 0.9984 - val_loss: 0.0482 - val_acc: 0.9887\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 155us/step - loss: 0.0221 - acc: 0.9983 - val_loss: 0.0515 - val_acc: 0.9879\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 166us/step - loss: 0.0204 - acc: 0.9985 - val_loss: 0.0459 - val_acc: 0.9879\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 166us/step - loss: 0.0187 - acc: 0.9987 - val_loss: 0.0476 - val_acc: 0.9865\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 3s 141us/step - loss: 0.0188 - acc: 0.9983 - val_loss: 0.0509 - val_acc: 0.9861\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 147us/step - loss: 0.0199 - acc: 0.9978 - val_loss: 0.0501 - val_acc: 0.9866\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 146us/step - loss: 0.0370 - acc: 0.9906 - val_loss: 0.1206 - val_acc: 0.9591\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 144us/step - loss: 0.0660 - acc: 0.9799 - val_loss: 0.0540 - val_acc: 0.9844\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 140us/step - loss: 0.0279 - acc: 0.9948 - val_loss: 0.0525 - val_acc: 0.9856\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 140us/step - loss: 0.0197 - acc: 0.9975 - val_loss: 0.0458 - val_acc: 0.9861\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 170us/step - loss: 0.0133 - acc: 0.9991 - val_loss: 0.0390 - val_acc: 0.9900\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 143us/step - loss: 0.0121 - acc: 0.9992 - val_loss: 0.0372 - val_acc: 0.9893\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 191us/step - loss: 0.0121 - acc: 0.9993 - val_loss: 0.0356 - val_acc: 0.9908\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 156us/step - loss: 0.0123 - acc: 0.9988 - val_loss: 0.2577 - val_acc: 0.9324\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 137us/step - loss: 0.1558 - acc: 0.9525 - val_loss: 0.0524 - val_acc: 0.9842\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 169us/step - loss: 0.0148 - acc: 0.9989 - val_loss: 0.0354 - val_acc: 0.9902\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 171us/step - loss: 0.0107 - acc: 0.9995 - val_loss: 0.0340 - val_acc: 0.9901\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 145us/step - loss: 0.0096 - acc: 0.9997 - val_loss: 0.0316 - val_acc: 0.9910\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 133us/step - loss: 0.0089 - acc: 0.9997 - val_loss: 0.0331 - val_acc: 0.9914\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 142us/step - loss: 0.0081 - acc: 0.9999 - val_loss: 0.0318 - val_acc: 0.9904\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 137us/step - loss: 0.0080 - acc: 0.9997 - val_loss: 0.0314 - val_acc: 0.9915\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 176us/step - loss: 0.0075 - acc: 0.9998 - val_loss: 0.0307 - val_acc: 0.9905\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 194us/step - loss: 0.0073 - acc: 0.9997 - val_loss: 0.0319 - val_acc: 0.9890\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 192us/step - loss: 0.0088 - acc: 0.9993 - val_loss: 0.0449 - val_acc: 0.9850\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 168us/step - loss: 0.0876 - acc: 0.9713 - val_loss: 0.0983 - val_acc: 0.9702\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 188us/step - loss: 0.0400 - acc: 0.9886 - val_loss: 0.0596 - val_acc: 0.9819\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 159us/step - loss: 0.0106 - acc: 0.9988 - val_loss: 0.0340 - val_acc: 0.9904\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 138us/step - loss: 0.0070 - acc: 0.9997 - val_loss: 0.0290 - val_acc: 0.9906\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 157us/step - loss: 0.0063 - acc: 0.9998 - val_loss: 0.0331 - val_acc: 0.9909\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 172us/step - loss: 0.0060 - acc: 0.9998 - val_loss: 0.0283 - val_acc: 0.9920\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 164us/step - loss: 0.0053 - acc: 0.9999 - val_loss: 0.0285 - val_acc: 0.9911\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 171us/step - loss: 0.0054 - acc: 0.9998 - val_loss: 0.0311 - val_acc: 0.9912\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 143us/step - loss: 0.0058 - acc: 0.9997 - val_loss: 0.0292 - val_acc: 0.9912\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 141us/step - loss: 0.0048 - acc: 0.9998 - val_loss: 0.0278 - val_acc: 0.9917\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 146us/step - loss: 0.0057 - acc: 0.9995 - val_loss: 0.0413 - val_acc: 0.9870\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 139us/step - loss: 0.1730 - acc: 0.9475 - val_loss: 0.0640 - val_acc: 0.9770\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 138us/step - loss: 0.0162 - acc: 0.9972 - val_loss: 0.0297 - val_acc: 0.9907\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 143us/step - loss: 0.0066 - acc: 0.9999 - val_loss: 0.0263 - val_acc: 0.9922\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 136us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0250 - val_acc: 0.9924\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 168us/step - loss: 0.0047 - acc: 0.9999 - val_loss: 0.0239 - val_acc: 0.9927\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 4s 200us/step - loss: 0.0045 - acc: 0.9999 - val_loss: 0.0242 - val_acc: 0.9925\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 151us/step - loss: 0.0042 - acc: 0.9999 - val_loss: 0.0236 - val_acc: 0.9928\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 144us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0231 - val_acc: 0.9931\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 187us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0235 - val_acc: 0.9929\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 176us/step - loss: 0.0036 - acc: 0.9999 - val_loss: 0.0235 - val_acc: 0.9925\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 144us/step - loss: 0.0037 - acc: 0.9999 - val_loss: 0.0243 - val_acc: 0.9919\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 156us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0244 - val_acc: 0.9922\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 140us/step - loss: 0.0058 - acc: 0.9991 - val_loss: 0.1233 - val_acc: 0.9590\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 3s 142us/step - loss: 0.1271 - acc: 0.9588 - val_loss: 0.0561 - val_acc: 0.9806\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 126us/step - loss: 0.0103 - acc: 0.9988 - val_loss: 0.0284 - val_acc: 0.9902\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 135us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0243 - val_acc: 0.9914\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 124us/step - loss: 0.0041 - acc: 0.9999 - val_loss: 0.0261 - val_acc: 0.9918\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 2s 124us/step - loss: 0.0038 - acc: 0.9999 - val_loss: 0.0223 - val_acc: 0.9921\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 145us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0231 - val_acc: 0.9927\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 186us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0246 - val_acc: 0.9914\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 4s 203us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0208 - val_acc: 0.9931\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 190us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0223 - val_acc: 0.9928\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 169us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0204 - val_acc: 0.9931\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 155us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0236 - val_acc: 0.9930\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "18000/18000 [==============================] - 3s 139us/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.0366 - val_acc: 0.9880\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(100):\n",
    "    #print()\n",
    "    #print('-' * 50)\n",
    "    #print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    #for i in range(1):\n",
    "        #ind = np.random.randint(0, len(x_val))\n",
    "        #rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        #preds = model.predict_classes(rowx, verbose=0)\n",
    "        #q = ctable.decode(rowx[0])\n",
    "        #correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        #print('Q', q[::-1] if REVERSE else q, end=' ')\n",
    "        #print('T', correct, end=' ')\n",
    "        #if correct == guess:\n",
    "            #print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        #else:\n",
    "            #print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        #print(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練好的模型拿來預測測試集，可以看到隨機抽取的預測結果基本上都是對的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG : Prediction\n"
     ]
    }
   ],
   "source": [
    "print(\"MSG : Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 84+752  T 836  \u001b[92m☑\u001b[0m 836 \n",
      "Q 38+994  T 1032 \u001b[92m☑\u001b[0m 1032\n",
      "Q 128+375 T 503  \u001b[92m☑\u001b[0m 503 \n",
      "Q 66+646  T 712  \u001b[92m☑\u001b[0m 712 \n",
      "Q 722+249 T 971  \u001b[92m☑\u001b[0m 971 \n",
      "Q 35+569  T 604  \u001b[92m☑\u001b[0m 604 \n",
      "Q 68+226  T 294  \u001b[92m☑\u001b[0m 294 \n",
      "Q 777+712 T 1489 \u001b[92m☑\u001b[0m 1489\n",
      "Q 88+971  T 1059 \u001b[92m☑\u001b[0m 1059\n",
      "Q 33+810  T 843  \u001b[92m☑\u001b[0m 843 \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "        ind = np.random.randint(0, len(test_x))\n",
    "        rowx, rowy = test_x[np.array([ind])], test_y[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------減法器---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation (subtractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成減法的資料，把加號改成減號即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 80000\n"
     ]
    }
   ],
   "source": [
    "questions1 = []\n",
    "expected1 = []\n",
    "seen1 = set()\n",
    "print('Generating data...')\n",
    "while len(questions1) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen1:\n",
    "        continue\n",
    "    seen1.add(key)\n",
    "    q = '{}-{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a - b)\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    #if REVERSE:\n",
    "    #    query = query[::-1]\n",
    "    questions1.append(query)\n",
    "    expected1.append(ans)\n",
    "print('Total addition questions:', len(questions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7-77   ', '8-1    ', '952-21 ', '8-0    ', '166-57 '] ['-70 ', '7   ', '931 ', '8   ', '109 ']\n"
     ]
    }
   ],
   "source": [
    "print(questions1[:5], expected1[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 經測試後發現減法器在其他條件不變下需要更大的資料量才能提升準確率，\n",
    "## 因此將訓練集數量增加到45000筆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x1 = np.zeros((len(questions1), MAXLEN, len(chars2)), dtype=np.bool)\n",
    "y1 = np.zeros((len(expected1), DIGITS + 1, len(chars2)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions1):\n",
    "    x1[i] = ctable2.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected1):\n",
    "    y1[i] = ctable2.encode(sentence, DIGITS + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n",
      "Testing Data:\n",
      "(30000, 7, 12)\n",
      "(30000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "indices1 = np.arange(len(y1))\n",
    "np.random.shuffle(indices1)\n",
    "x1 = x1[indices1]\n",
    "y1 = y1[indices1]\n",
    "\n",
    "# train_test_split\n",
    "train_x1 = x1[:50000]\n",
    "train_y1 = y1[:50000]\n",
    "test_x1 = x1[50000:]\n",
    "test_y1 = y1[50000:]\n",
    "\n",
    "split_at = len(train_x1) - len(train_x1) // 10\n",
    "(x_train1, x_val1) = train_x1[:split_at], train_x1[split_at:]\n",
    "(y_train1, y_val1) = train_y1[:split_at], train_y1[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train1.shape)\n",
    "print(y_train1.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val1.shape)\n",
    "print(y_val1.shape)\n",
    "\n",
    "print('Testing Data:')\n",
    "print(test_x1.shape)\n",
    "print(test_y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_5 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 4, 12)             1548      \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars2))))\n",
    "model1.add(layers.RepeatVector(DIGITS + 1))\n",
    "\n",
    "for _ in range(LAYERS):\n",
    "    model1.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "model1.add(layers.TimeDistributed(layers.Dense(len(chars2), activation='softmax')))\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(100):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model1.fit(x_train1, y_train1,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val1, y_val1))\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val1))\n",
    "        rowx, rowy = x_val1[np.array([ind])], y_val1[np.array([ind])]\n",
    "        preds = model1.predict_classes(rowx, verbose=0)\n",
    "        q = ctable2.decode(rowx[0])\n",
    "        correct = ctable2.decode(rowy[0])\n",
    "        guess = ctable2.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model1.predict_classes(test_x1)\n",
    "\n",
    "count = 0\n",
    "for i in range(len(preds)):\n",
    "    \n",
    "    q = ctable2.decode(test_x1[i])\n",
    "    correct = ctable2.decode(test_y1[i])\n",
    "    guess = ctable2.decode(preds[i], calc_argmax=False)\n",
    "    \n",
    "    print('Q', q, end=' ')\n",
    "    print('T', correct, end=' ')\n",
    "    if correct == guess:\n",
    "        print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        count += 1\n",
    "    else:\n",
    "        print(colors.fail + '☒' + colors.close, end=' ')\n",
    "    print(guess)\n",
    "    \n",
    "print(count/len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以測試集測試減法器準確率可以達到96.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy :\n",
      "0.9694666666666667\n"
     ]
    }
   ],
   "source": [
    "print('the accuracy :')\n",
    "print(count/len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------加法+減法------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合併加法跟減法的資料，訓練一個既可以學習加法也可以學習減法的運算模型\n",
    "### 而因為資料變異較大，需要用更大量的資料來訓練，因此訓練集給了80000筆資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 80000\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "\n",
    "print('Total addition questions:', len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(questions) < 160000:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    q = '{}-{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a - b)\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    #if REVERSE:\n",
    "        #query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5+1    ', '9+151  ', '152+314', '693+726', '8+72   '] ['6   ', '160 ', '466 ', '1419', '80  ']\n",
      "['209-39 ', '858-69 ', '611-728', '71-372 ', '165-961'] ['170 ', '789 ', '-117', '-301', '-796']\n"
     ]
    }
   ],
   "source": [
    "print(questions[:5],expected[:5])\n",
    "print(questions[159995:],expected[159995:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x_ = np.zeros((len(questions), MAXLEN, len(chars3)), dtype=np.bool)\n",
    "y_ = np.zeros((len(expected), DIGITS + 1, len(chars3)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x_[i] = ctable3.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y_[i] = ctable3.encode(sentence, DIGITS + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "(72000, 7, 13)\n",
      "(72000, 4, 13)\n",
      "Validation Data:\n",
      "(8000, 7, 13)\n",
      "(8000, 4, 13)\n",
      "Testing Data:\n",
      "(80000, 7, 13)\n",
      "(80000, 4, 13)\n"
     ]
    }
   ],
   "source": [
    "indices_ = np.arange(len(y_))\n",
    "np.random.shuffle(indices_)\n",
    "x_ = x_[indices_]\n",
    "y_ = y_[indices_]\n",
    "\n",
    "# train_test_split\n",
    "train_x_ = np.concatenate((x_[:40000],x_[120000:]),axis=0)\n",
    "train_y_ = np.concatenate((y_[:40000],y_[120000:]),axis=0)\n",
    "test_x_ = x_[40001:120001]\n",
    "test_y_ = y_[40001:120001]\n",
    "\n",
    "split_at = len(train_x_) - len(train_x_) // 10\n",
    "(x_train_, x_val_) = train_x_[:split_at], train_x_[split_at:]\n",
    "(y_train_, y_val_) = train_y_[:split_at], train_y_[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train_.shape)\n",
    "print(y_train_.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val_.shape)\n",
    "print(y_val_.shape)\n",
    "\n",
    "print('Testing Data:')\n",
    "print(test_x_.shape)\n",
    "print(test_y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 128)               72704     \n",
      "_________________________________________________________________\n",
      "repeat_vector_7 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 4, 13)             1677      \n",
      "=================================================================\n",
      "Total params: 205,965\n",
      "Trainable params: 205,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model11 = Sequential()\n",
    "model11.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars3))))\n",
    "model11.add(layers.RepeatVector(DIGITS + 1))\n",
    "\n",
    "for _ in range(LAYERS):\n",
    "    model11.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "model11.add(layers.TimeDistributed(layers.Dense(len(chars3), activation='softmax')))\n",
    "model11.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model11.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(100):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model11.fit(x_train_, y_train_,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val_, y_val_))\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val_))\n",
    "        rowx, rowy = x_val_[np.array([ind])], y_val_[np.array([ind])]\n",
    "        preds = model11.predict_classes(rowx, verbose=0)\n",
    "        q = ctable3.decode(rowx[0])\n",
    "        correct = ctable3.decode(rowy[0])\n",
    "        guess = ctable3.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試後準確率可以達到96.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ = model11.predict_classes(test_x_)\n",
    "\n",
    "count_ = 0\n",
    "for i in range(len(preds_)):\n",
    "    \n",
    "    q = ctable3.decode(test_x_[i])\n",
    "    correct = ctable3.decode(test_y_[i])\n",
    "    guess = ctable3.decode(preds_[i], calc_argmax=False)\n",
    "    \n",
    "    print('Q', q, end=' ')\n",
    "    print('T', correct, end=' ')\n",
    "    if correct == guess:\n",
    "        print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        count_ += 1\n",
    "    else:\n",
    "        print(colors.fail + '☒' + colors.close, end=' ')\n",
    "    print(guess)\n",
    "    \n",
    "print(count_/len(preds_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy :\n",
      "0.9689625\n"
     ]
    }
   ],
   "source": [
    "print('the accuracy :')\n",
    "print(count_/len(preds_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
